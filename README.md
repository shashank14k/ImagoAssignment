# Model Training, Evaluation, and Prediction System

## Setup
The below commands will create a venv named imago and install the requirements. This can then be activated using imago/bin/activate
```commandline
chmod +x build.sh
./build.sh
source imago/bin/activate
```

This repository contains three main components for training, evaluating, and serving a machine learning model.

## 1. `trainer.py` (Training the Model)

- **Inputs:**
  - `config_file` (YAML): Specifies training hyperparameters, model architecture, and preprocessing settings.
  - `input_file` (CSV): The dataset used for training, with target and feature columns.
  - `device` (string, optional): Defines whether training should run on `"cuda"`, `"cpu"`, or `"mps"`.
- **Functionality:**
  - Preprocesses the dataset (handles missing values, outliers, and feature scaling).
  - Initializes and trains a model using K-Fold cross-validation.
  - Saves the best model (`best_model.pth`) in the log dir specified in config.

## 2. `evaluate.py` (Evaluating the Model)

- **Inputs:**
  - `config` (YAML): Path to the model configuration.
  - `checkpoint` (model `.pth` file): Trained model weights.
  - `train` (CSV): Training dataset.
  - `test` (CSV): Test dataset.
  - `report` (DOCX, optional): Output path for the evaluation report.
- **Functionality:**
  - Computes regression metrics: RMSE, MAE, R².
  - Generates SHAP (SHapley Additive Explanations) plots for feature importance.
  - Computes Mutual Information Regression (MIR) scores for feature to evaluate the importance of selected features
  - Outputs a comprehensive model evaluation report in `.docx` format.

## 3. `app.py` (Serving Predictions via FastAPI)
- **Functionality:**
  - Loads the trained model dynamically based on `model_type`.
  - Provides two API endpoints:
    - `POST /predict` → Single sample inference.
    - `POST /batch_predict` → Batch inference with multiple samples.

## 4. `src/preprocess/analyze.py` (Dataset Analysis)
- **Inputs:**
  - `--data` (CSV): Path to the dataset CSV file.
  - `--report` (DOCX, optional): Output report path (default: `analysis_report.docx`).
  - `--alpha` (float, optional): Significance level for normality tests (default: 0.05).
  - `--top_n` (int, optional): Number of top items to display in analyses (default: 10).
  - `--pca_var` (float, optional): Variance to retain in PCA (default: 0.95).
- **Functionality:**
  - Performs null value analysis, correlation analysis, and normality tests.
  - Detects outliers using IQR and generates visualization reports.
  - Computes PCA for dimensionality reduction and visualizes feature importance.
  - Outputs a `.docx` report summarizing the dataset analysis.

## 5. `src/preprocess/preprocess.py` (Data Preprocessing)
- **Inputs:**
  - `--csv_file` (CSV): Path to the input CSV file.
  - `--target_col` (str): Name of the target column.
  - `--ignore_columns` (list, optional): Columns to ignore.
  - `--null_method` (str, optional): Method for handling missing values (`median`, `mean`, `None`).
  - `--iqr_factor` (float, optional): IQR multiplier for outlier detection (default: 1.5).
  - `--low_q` (float, optional): Lower quantile for outlier detection (default: 0.2).
  - `--high_q` (float, optional): Upper quantile for outlier detection (default: 0.8).
  - `--outlier_method` (str, optional): Outlier treatment method (`clip`, `mean`).
  - `--include_target` (flag, optional): Include target column in outlier treatment.
  - `--n_components` (float, optional): Fraction of PCA components to retain (default: 0.2).
  - `--output_file` (CSV): Path to save transformed CSV file.
- **Functionality:**
  - Fills missing values based on the specified method.
  - Detects and treats outliers using IQR-based filtering.
  - Applies PCA for feature selection and scales the dataset.
  - Outputs a preprocessed CSV file for model training.

## Usage

### 1. Train the Model

```bash
python trainer.py --config_file configs/conf.yaml --input_file data/train.csv --device cuda
```

### 2. Evaluate the Model

```bash
python evaluate.py --config configs/conf.yaml --checkpoint logs/1/best_model.pth --train data/train.csv --test data/test.csv
```

### 3. Run the API Server

```bash
python app.py --host 0.0.0.0 --port 8000 --device cuda
```

### 4. Create analysis report on dataframe
```bash
python src/preprocess/analyze.py --data path/to/input.csv --report path/to/output.docx
```

### 5. Preprocess a csv file
```bash
python src/preprocess/preprocess.py --csv_file path/to/input.csv --target_col target_column --output_file path/to/output.csv
```

### Key Attached Reports
  
  1. FinalReport.docx -> Short summary of key findings
2. analysis_report.docx -> doc file summarizing MLE-Assignment.csv
3. transformed_df_analysis.docx -> doc file summarizing transformed_df.csv, generated by treating outliers in original csv and limiting features
4. baseline_report.docx -> doc file evaluating baseline regression model
5. baseline_transformer.docx -> doc file evaluating transformer network

### Run tests
```commandline
pytest tests
```
